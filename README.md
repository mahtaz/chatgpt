# chatgpt

This notebook involves building a bigram language model using the Transformer architecture, which is composed of four main components: the Head, the Multihead, the feed forward, and the Block.

Overall, this project aims to develop an  language model that can generate meaningful text by leveraging the power of the Transformer architecture which consists of several components:

- `Head` is a single self-attention block that performs attention on the input sequence to produce a representation that captures the dependencies between the tokens in the sequence.

- `MultiHead Attention` combines multiple Heads to form a multi-head attention block, which enables the model to attend to different parts of the input sequence at the same time.

- `FeedFoward` The feedforward layer typically consists of two linear transformations separated by an activation function, such as ReLU. The first linear transformation projects the output of the multi-head attention module to a higher-dimensional space, while the second linear transformation projects it back to the original dimension. The activation function is applied element-wise, providing non-linearity to the output. This process enables the model to learn more complex feature representations that capture higher-order interactions between the input tokens, leading to improved performance on a range of natural language processing tasks.

- `Block` the Block component contains the Multihead component, a layer normalization layer, an embedding layer, and a feedforward layer. The embedding layer maps the input tokens to a continuous vector space, while the feedforward layer applies a non-linear transformation to the representations generated by the Multihead component. The layer normalization layer helps to stabilize the training process by normalizing the activations of the previous layer

For a detailed explanation run this [notebook](https://github.com/mahtaz/chatgpt/blob/main/chatgpt_vanilla.ipynb)
<p align="center">
  <img src="https://github.com/mahtaz/chatgpt/blob/main/images/transformer%20(2).jpg" alt="tranformer" height="500">
</p>

## Brief summary about Attention

Attention involves answering what part of the input should I focus on. For example in the case of translation We want to answer how relevant is the word in the
sentence relevant to other words in the same sentence for every word. We can have an attention vector generated which captures contextual relationships
between words in the sentence as shown below.

<p align="center">
  <img src="https://github.com/mahtaz/chatgpt/blob/main/images/attention%20(2).jpg" alt="tranformer" height="400">
</p>

### Libraries

- pytorch
- matplotlib
- numpy

## References

1. [nanogpt-lecture](https://github.com/karpathy/ng-video-lecture)
2. [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
3. [Transformers and Multi-Head Attention](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html)
