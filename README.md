# chatgpt
This notebook contains transformer-based bigram language model that generates text which consists of several components:
* `Head` is a single head of the self-attention mechanism.
* `MultiHead Attention` uses multiple heads of self-attention in parallel.
* `FeedFoward` defines a linear layer followed by a non-linearity.
* `Block` is the Transformer block, which performs communication followed by computation.
* `BigramLanguageModel` is the language model that uses the Transformer architecture.
